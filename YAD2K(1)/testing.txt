train
train
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 416, 416, 3)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 416, 416, 32) 864         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 416, 416, 32) 128         conv2d_1[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)       (None, 416, 416, 32) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 208, 208, 32) 0           leaky_re_lu_1[0][0]              
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 208, 208, 64) 18432       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 208, 208, 64) 256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)       (None, 208, 208, 64) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 104, 104, 64) 0           leaky_re_lu_2[0][0]              
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 104, 104, 128 73728       max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 104, 104, 128 512         conv2d_3[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)       (None, 104, 104, 128 0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 104, 104, 64) 8192        leaky_re_lu_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 104, 104, 64) 256         conv2d_4[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)       (None, 104, 104, 64) 0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 104, 104, 128 73728       leaky_re_lu_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 104, 104, 128 512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)       (None, 104, 104, 128 0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 52, 52, 128)  0           leaky_re_lu_5[0][0]              
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 52, 52, 256)  294912      max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 52, 52, 256)  1024        conv2d_6[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)       (None, 52, 52, 256)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 52, 52, 128)  32768       leaky_re_lu_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 52, 52, 128)  512         conv2d_7[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)       (None, 52, 52, 128)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 52, 52, 256)  294912      leaky_re_lu_7[0][0]              
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 52, 52, 256)  1024        conv2d_8[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)       (None, 52, 52, 256)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 26, 26, 256)  0           leaky_re_lu_8[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 26, 26, 512)  1179648     max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 26, 26, 512)  2048        conv2d_9[0][0]                   
__________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)       (None, 26, 26, 512)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_9[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 26, 26, 256)  1024        conv2d_10[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_10[0][0]             
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 26, 26, 512)  2048        conv2d_11[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 26, 26, 256)  131072      leaky_re_lu_11[0][0]             
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 26, 26, 256)  1024        conv2d_12[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 26, 26, 512)  1179648     leaky_re_lu_12[0][0]             
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 26, 26, 512)  2048        conv2d_13[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)      (None, 26, 26, 512)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_13[0][0]             
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 13, 13, 1024) 4718592     max_pooling2d_5[0][0]            
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 13, 13, 1024) 4096        conv2d_14[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 13, 13, 512)  524288      leaky_re_lu_14[0][0]             
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 13, 13, 512)  2048        conv2d_15[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 13, 13, 1024) 4718592     leaky_re_lu_15[0][0]             
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 13, 13, 1024) 4096        conv2d_16[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 13, 13, 512)  524288      leaky_re_lu_16[0][0]             
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 13, 13, 512)  2048        conv2d_17[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)      (None, 13, 13, 512)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 13, 13, 1024) 4718592     leaky_re_lu_17[0][0]             
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 13, 13, 1024) 4096        conv2d_18[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 13, 13, 1024) 9437184     leaky_re_lu_18[0][0]             
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 13, 13, 1024) 4096        conv2d_19[0][0]                  
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 26, 26, 64)   32768       leaky_re_lu_13[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 26, 26, 64)   256         conv2d_21[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 13, 13, 1024) 9437184     leaky_re_lu_19[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)      (None, 26, 26, 64)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 13, 13, 1024) 4096        conv2d_20[0][0]                  
__________________________________________________________________________________________________
space_to_depth (Lambda)         (None, 13, 13, 256)  0           leaky_re_lu_21[0][0]             
__________________________________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 13, 13, 1280) 0           space_to_depth[0][0]             
                                                                 leaky_re_lu_20[0][0]             
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 13, 13, 1024) 11796480    concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 13, 13, 1024) 4096        conv2d_22[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_22 (LeakyReLU)      (None, 13, 13, 1024) 0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 13, 13, 36)   36900       leaky_re_lu_22[0][0]             
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, None, 5)      0                                            
__________________________________________________________________________________________________
input_3 (InputLayer)            (None, 13, 13, 4, 1) 0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 13, 13, 4, 5) 0                                            
__________________________________________________________________________________________________
yolo_loss (Lambda)              (None, 1)            0           conv2d_24[0][0]                  
                                                                 input_2[0][0]                    
                                                                 input_3[0][0]                    
                                                                 input_4[0][0]                    
==================================================================================================
Total params: 50,584,836
Trainable params: 36,900
Non-trainable params: 50,547,936
__________________________________________________________________________________________________
Train on 630 samples, validate on 70 samples
Epoch 1/5

 32/630 [>.............................] - ETA: 39s - loss: 80.9710
 64/630 [==>...........................] - ETA: 21s - loss: 75.4284
 96/630 [===>..........................] - ETA: 15s - loss: 71.0641
128/630 [=====>........................] - ETA: 12s - loss: 67.8650
160/630 [======>.......................] - ETA: 10s - loss: 65.2576
192/630 [========>.....................] - ETA: 8s - loss: 66.1936 
224/630 [=========>....................] - ETA: 7s - loss: 65.2603
256/630 [===========>..................] - ETA: 6s - loss: 67.4625
288/630 [============>.................] - ETA: 5s - loss: 66.6590
320/630 [==============>...............] - ETA: 4s - loss: 67.2564
352/630 [===============>..............] - ETA: 4s - loss: 65.5863
384/630 [=================>............] - ETA: 3s - loss: 64.1639
416/630 [==================>...........] - ETA: 3s - loss: 64.6195
448/630 [====================>.........] - ETA: 2s - loss: 64.8894
480/630 [=====================>........] - ETA: 2s - loss: 64.3507
512/630 [=======================>......] - ETA: 1s - loss: 64.9956
544/630 [========================>.....] - ETA: 1s - loss: 64.4358
576/630 [==========================>...] - ETA: 0s - loss: 64.3878
608/630 [===========================>..] - ETA: 0s - loss: 63.2061
630/630 [==============================] - 10s 16ms/step - loss: 62.2434 - val_loss: 60.8924
Epoch 2/5

 32/630 [>.............................] - ETA: 5s - loss: 39.9234
 64/630 [==>...........................] - ETA: 5s - loss: 55.3677
 96/630 [===>..........................] - ETA: 5s - loss: 56.5016
128/630 [=====>........................] - ETA: 4s - loss: 56.5174
160/630 [======>.......................] - ETA: 4s - loss: 55.1514
192/630 [========>.....................] - ETA: 4s - loss: 52.6515
224/630 [=========>....................] - ETA: 4s - loss: 54.0831
256/630 [===========>..................] - ETA: 3s - loss: 55.0704
288/630 [============>.................] - ETA: 3s - loss: 55.4475
320/630 [==============>...............] - ETA: 3s - loss: 54.5934
352/630 [===============>..............] - ETA: 2s - loss: 54.4541
384/630 [=================>............] - ETA: 2s - loss: 55.1967
416/630 [==================>...........] - ETA: 2s - loss: 54.8439
448/630 [====================>.........] - ETA: 1s - loss: 54.6468
480/630 [=====================>........] - ETA: 1s - loss: 54.6545
512/630 [=======================>......] - ETA: 1s - loss: 53.7956
544/630 [========================>.....] - ETA: 0s - loss: 54.1079
576/630 [==========================>...] - ETA: 0s - loss: 55.5645
608/630 [===========================>..] - ETA: 0s - loss: 56.3234
630/630 [==============================] - 7s 11ms/step - loss: 55.8377 - val_loss: 57.7035
Epoch 3/5

 32/630 [>.............................] - ETA: 5s - loss: 61.1442
 64/630 [==>...........................] - ETA: 5s - loss: 52.2379
 96/630 [===>..........................] - ETA: 5s - loss: 51.9678
128/630 [=====>........................] - ETA: 4s - loss: 55.8355
160/630 [======>.......................] - ETA: 4s - loss: 55.6148
192/630 [========>.....................] - ETA: 4s - loss: 58.1702
224/630 [=========>....................] - ETA: 3s - loss: 56.8261
256/630 [===========>..................] - ETA: 3s - loss: 58.0233
288/630 [============>.................] - ETA: 3s - loss: 56.9973
320/630 [==============>...............] - ETA: 3s - loss: 56.1799
352/630 [===============>..............] - ETA: 2s - loss: 55.7478
384/630 [=================>............] - ETA: 2s - loss: 56.3580
416/630 [==================>...........] - ETA: 2s - loss: 56.3790
448/630 [====================>.........] - ETA: 1s - loss: 55.0433
480/630 [=====================>........] - ETA: 1s - loss: 54.4134
512/630 [=======================>......] - ETA: 1s - loss: 55.3328
544/630 [========================>.....] - ETA: 0s - loss: 54.9731
576/630 [==========================>...] - ETA: 0s - loss: 55.1696
608/630 [===========================>..] - ETA: 0s - loss: 54.8776
630/630 [==============================] - 7s 11ms/step - loss: 54.4570 - val_loss: 56.7716
Epoch 4/5

 32/630 [>.............................] - ETA: 5s - loss: 44.5620
 64/630 [==>...........................] - ETA: 5s - loss: 51.9460
 96/630 [===>..........................] - ETA: 5s - loss: 51.9209
128/630 [=====>........................] - ETA: 4s - loss: 51.6580
160/630 [======>.......................] - ETA: 4s - loss: 51.9016
192/630 [========>.....................] - ETA: 4s - loss: 51.0282
224/630 [=========>....................] - ETA: 3s - loss: 50.8803
256/630 [===========>..................] - ETA: 3s - loss: 53.1503
288/630 [============>.................] - ETA: 3s - loss: 53.7693
320/630 [==============>...............] - ETA: 3s - loss: 54.5744
352/630 [===============>..............] - ETA: 2s - loss: 53.5752
384/630 [=================>............] - ETA: 2s - loss: 53.2727
416/630 [==================>...........] - ETA: 2s - loss: 52.8803
448/630 [====================>.........] - ETA: 1s - loss: 53.1845
480/630 [=====================>........] - ETA: 1s - loss: 54.0905
512/630 [=======================>......] - ETA: 1s - loss: 55.1635
544/630 [========================>.....] - ETA: 0s - loss: 54.5910
576/630 [==========================>...] - ETA: 0s - loss: 54.8310
608/630 [===========================>..] - ETA: 0s - loss: 54.0068
630/630 [==============================] - 7s 11ms/step - loss: 53.2763 - val_loss: 55.3947
Epoch 5/5

 32/630 [>.............................] - ETA: 5s - loss: 67.0516
 64/630 [==>...........................] - ETA: 5s - loss: 51.4458
 96/630 [===>..........................] - ETA: 5s - loss: 57.1493
128/630 [=====>........................] - ETA: 4s - loss: 55.7644
160/630 [======>.......................] - ETA: 4s - loss: 55.2533
192/630 [========>.....................] - ETA: 4s - loss: 54.6398
224/630 [=========>....................] - ETA: 4s - loss: 53.2575
256/630 [===========>..................] - ETA: 3s - loss: 52.8611
288/630 [============>.................] - ETA: 3s - loss: 54.5396
320/630 [==============>...............] - ETA: 3s - loss: 53.3714
352/630 [===============>..............] - ETA: 2s - loss: 52.7194
384/630 [=================>............] - ETA: 2s - loss: 52.8591
416/630 [==================>...........] - ETA: 2s - loss: 53.6007
448/630 [====================>.........] - ETA: 1s - loss: 53.9579
480/630 [=====================>........] - ETA: 1s - loss: 53.7470
512/630 [=======================>......] - ETA: 1s - loss: 53.1091
544/630 [========================>.....] - ETA: 0s - loss: 52.4581
576/630 [==========================>...] - ETA: 0s - loss: 52.8133
608/630 [===========================>..] - ETA: 0s - loss: 52.0726
630/630 [==============================] - 7s 11ms/step - loss: 51.3566 - val_loss: 53.6453
